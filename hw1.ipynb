{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunwenFang/1015-Text-as-Data/blob/main/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework 1 (36 points)\n",
        "\n",
        "*Due 11:59pm, February 28, 2025 through Brightspace*\n",
        "\n",
        "Based on material from\n",
        "\n",
        "- Lectures: Week 1, Week 2, Week 3 (up to Slide 53)\n",
        "- Lab sessions: Recitations 0, 1, 2\n",
        "\n",
        "Required files (both on Brightspace)\n",
        "\n",
        "- wiki40b.txt\n",
        "- universal_dependencies.txt\n",
        "\n",
        "As this is the first homework assignment, please be sure to read all the instructions especially carefully.\n",
        "\n",
        "1. All the work must be your own. You must not copy anyone's work, or allow anyone to copy yours (both the code and written responses). You may discuss the assignment with others, but you must prepare the responses alone.\n",
        "2. You should 1) create a copy of this notebook, 2) rename it as f\"hw1_{your_netid}.ipynb\" (e.g. hw1_aqe69420.ipynb), 3) prepare the responses, 4) download it as **.ipynb** (with all code output), and 5) submit it on Brightspace. Not following these formatting instructions greatly slows down our grading speed...\n",
        "3. Your code must be included in full so that we can assess your understanding of the problems. Note that code comments will not be graded.\n",
        "4. This notebook contains all the code/text blocks you need to complete the assignment. If you've added/deleted blocks accidentally, just refer back to this original document.\n",
        "\n",
        "\n",
        "Please see below for how an example question and response might be formatted (you are expected to add your answer after every blockquote character \\>).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "X5xFV78_9hst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Question\n",
        "\n",
        "Split the sentence *New York University (NYU) is a private research university in New York City, New York* by whitespace and report how many unique kinds of whitespace-delimited words there are in the sentence (2 points; 1 point for answer + 1 point for code)."
      ],
      "metadata": {
        "id": "UvVD5NlNEIaq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0QPsT-w9d_o",
        "outputId": "b04458c6-cd5b-4c4e-937d-cbc822300fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'in', 'City,', 'New', '(NYU)', 'a', 'research', 'private', 'University', 'university', 'York', 'is'}\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "# code for Example Question\n",
        "\n",
        "def split_string(my_string):\n",
        "  return set(my_string.split())\n",
        "\n",
        "nyu = split_string(\"New York University (NYU) is a private research university in New York City, New York\")\n",
        "print(nyu)\n",
        "print(len(nyu))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer to Example Question:\n",
        "> 11\n",
        "\n",
        "---\n",
        "#### Begin actual assignment:"
      ],
      "metadata": {
        "id": "C38PoYUhGBgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Information\n",
        "\n",
        "1. Please enter your name (as it appears on Brightspace).\n",
        ">\n",
        "\n",
        "2. Who did you collaborate with on this assignment (enter N/A if no one)?\n",
        ">\n",
        "\n",
        "3. Did you use generative AI tools, and if so, which tools and for what purpose (enter N/A if none)?\n",
        ">\n"
      ],
      "metadata": {
        "id": "i1alv_pcHp77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (6 points)\n",
        "\n",
        "Objective: Question 1 tests your understanding of basic definitions.\n",
        "\n",
        "Given two vectors $$\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{bmatrix}, \\ \\mathbf{y} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{bmatrix},$$\n",
        "\n",
        "1-1. Calculate the Euclidean distance between $\\mathbf{x}$ and $\\mathbf{y}$ (2 points; 1 point for answer + 1 point for code).\n",
        "\n",
        "1-2. Calculate the Manhattan distance between $\\mathbf{x}$ and $\\mathbf{y}$ (2 points; 1 point for answer + 1 point for code).\n",
        "\n",
        "1-3. Calculate the cosine similarity between $\\mathbf{x}$ and $\\mathbf{y}$ (2 points; 1 point for answer + 1 point for code).\n",
        "\n",
        "The use of libraries is **not allowed** for Question 1."
      ],
      "metadata": {
        "id": "sg7_g6WfHBSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use this block for 1-1, 1-2, and 1-3\n",
        "# you can write a function to calculate the quantities,\n",
        "# or write out the equation and evaluate the answer.\n",
        "\n",
        "# code for 1-1\n",
        "# Define vectors x and y\n",
        "x = [1, 3, 5, 7]\n",
        "y = [2, 4, 6, 8]\n",
        "\n",
        "euclidean_distance = ((x[0] - y[0])**2 + (x[1] - y[1])**2 + (x[2] - y[2])**2 + (x[3] - y[3])**2) ** 0.5\n",
        "\n",
        "# code for 1-2\n",
        "manhattan_distance = abs(x[0] - y[0]) + abs(x[1] - y[1]) + abs(x[2] - y[2]) + abs(x[3] - y[3])\n",
        "\n",
        "# code for 1-3\n",
        "dot_product = x[0]*y[0] + x[1]*y[1] + x[2]*y[2] + x[3]*y[3]\n",
        "magnitude_x = (x[0]**2 + x[1]**2 + x[2]**2 + x[3]**2) ** 0.5\n",
        "magnitude_y = (y[0]**2 + y[1]**2 + y[2]**2 + y[3]**2) ** 0.5\n",
        "cosine_similarity = dot_product / (magnitude_x * magnitude_y)\n",
        "\n",
        "print(euclidean_distance, manhattan_distance, cosine_similarity)\n",
        "\n"
      ],
      "metadata": {
        "id": "n7O_EfxNNSB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab81626b-ffed-47a5-de4b-e41e3e4a3262"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0 4 0.9960238411119947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer to 1-1:\n",
        "> 2\n",
        "\n",
        "Answer to 1-2:\n",
        "> 4\n",
        "\n",
        "Answer to 1-3:\n",
        "> 0.996"
      ],
      "metadata": {
        "id": "l2h8HRHNOIb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (8 points)\n",
        "\n",
        "Objective: Question 2 tests your skill to preprocess \"real\" text data, as might be required for your project. As mentioned in the lecture, text data is often noisy and can require more thought and work to preprocess than textbook examples.\n",
        "\n",
        "2-1. Preprocess the three Wikipedia articles in the provided \"wiki40b.txt\" and report the length of the resulting vocabulary, $|V|$. Follow these instructions (4 points; 1 point for answer + 3 points for code):\n",
        "- Preprocess only the paragraph text (no titles or headers)\n",
        "- No special tags (like *\\_NEWLINE\\_*) should be included\n",
        "- Apply lowercasing\n",
        "- Remove punctuation"
      ],
      "metadata": {
        "id": "SIU6WA3lNH1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for 2-1\n",
        "# the use of libraries is allowed\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # 运行后选择文件上传"
      ],
      "metadata": {
        "id": "s_U5hl9JQlkW",
        "outputId": "4cd86c49-a758-47eb-e1f8-b6936c352910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06c23aad-14a8-4db7-8351-6792ad31ac18\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06c23aad-14a8-4db7-8351-6792ad31ac18\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving universal_dependencies.txt to universal_dependencies (1).txt\n",
            "Saving wiki40b.txt to wiki40b (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/wiki40b.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "  text = file.read()\n",
        "text"
      ],
      "metadata": {
        "id": "gbLZMxbjY477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "2c3d2baa-36e5-4ca1-c522-02771061b265"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'_START_ARTICLE_\\nKasturba Road\\n_START_PARAGRAPH_\\nKasturba Road is a street in Bangalore, the capital of Karnataka, India, which is connected to M G Road to the north and J C Road to the south. Some important landmarks situated along Kasturba Road are Sree Kan\\nteerava Stadium, Kanteerava Indoor Stadium, Cubbon Park, Government Museum, Venkatappa Art Gallery, Visvesvaraya Industrial and Technological Museum and UB City. A 600-year-old Ganesha temple is also situated o\\nn Kasturba Road._NEWLINE_It was earlier known as Sydney Road._NEWLINE_Other important landmarks close to the road are Karnataka High Court, Vidhana Soudha and Chinnaswamy Stadium.\\n\\n_START_ARTICLE_\\nAmazon (yacht)\\n_START_SECTION_\\nConstruction\\n_START_PARAGRAPH_\\nCarvel planked in teak and pitch pine on oak frames, with alternate wrought iron strap floor reinforcement, bronze fastenings, lead keel and copper sheathing, the Amazon\\'s hull is still largely original.\\n_START_SECTION_\\nHistory\\n_START_PARAGRAPH_\\nHer builder and first owner, Tankerville Chamberlayne, an English gentleman, personally supervised her construction by his own Arrow Yard at Northam on the River Itchen.  This small private facility was establi\\nshed by the Chamberlayne family for the maintenance of the famous cutter Arrow, which was adapted continuously and thereby kept racing competitively into the 1890s.  Amazon\\'s engine and boiler were supplied by\\nthe adjacent works of Day, Summers and Company._NEWLINE_Amazon was used for summer cruising, to attend sailing regattas along the south coast of England, and to visit France. Having been prepared appropriately\\nfor the occasion of Queen Victoria\\'s Diamond Jubilee Royal Fleet Review in 1897 (at which Turbinia made her debut), she was shortly after sold to a prominent French yachtsman and was based at Saint Malo as Armo\\nricain until 1900, when she returned to British ownership._NEWLINE_Already too old (and with a coal-fired compound engine thought to be rather too old-fashioned) for the First World War, she remained in south c\\noast ports as a private yacht.  A new owner took her to London and after 52 years of service her original engine and boiler were removed on her conversion to diesel in 1937.  The Second World War put paid to pl\\neasure cruising and she subsequently became a houseboat for some years in a west London Yacht Basin._NEWLINE_The actor Arthur Lowe bought her as a houseboat in 1968 and, encouraged by his surveyor\\'s positive re\\nport, made her ready for sea again in 1971; at first a private yacht she then pursued a successful charter business in the 1980s, before migrating to northern Scotland in 1990._NEWLINE_In 1997 she made passage\\nfrom Scotland to Malta, where her new owners used her for cruising in the Mediterranean.  In 2009 Amazon crossed the Atlantic Ocean via the Cape Verde Islands  and travelled in the Caribbean and to Bermuda._NEW\\nLINE_She arrived at Newport, Rhode Island, United States from Bermuda on Labor Day 2009. Amazon was hosted by the Herreshoff Marine Museum at Bristol, Rhode Island in October 2009. She spent time in Narraganset\\nt Bay. The yacht subsequently travelled to Mystic Seaport in late 2009  and was based there in early 2011.  Amazon remained at Mystic Seaport until mid-2011._NEWLINE_Amazon acted as Flagship for the Commodore o\\nf the Mystic River Yacht Club for a charity regatta in Long Island Sound in June 2011 and visited Canada in July 2011 _NEWLINE_In August 2011 the yacht made a trans-Atlantic passage from Newfoundland to Ireland\\n, and arrived at Waterford on 2 September 2011 where she was described by a local boat owner as the \"classiest motor boat I have ever seen!\".  She remained at Waterford for the winter._NEWLINE_In May 2012 she v\\nisited Bristol before sailing to London, where she took part in the Thames Diamond Jubilee Pageant on Sunday 3 June 2012.  She was the only vessel present that had also witnessed the Diamond Jubilee Fleet Revie\\nw for Queen Victoria at Spithead on 26 June 1897. The Director of National Historic Ships referred to her in his public letter of criticism concerning the BBC\\'s coverage of the event._NEWLINE_She was subsequent\\nly at the Ramsgate Maritime Museum until late June, at Shoreham on 28 June 2012, then at Cowes and in the Bassin Vauban at St Malo, France in late July 2012._NEWLINE_In August and September 2012, Amazon was in\\nthe Channel Islands, visiting Alderney in August and Jersey in September, berthing in St Helier and Gorey Harbours; on 13 September she was in St Aubin\\'s Bay to watch the 2012 Jersey International Air Display._\\nNEWLINE_She was in Bristol during the winter and at the Southampton Maritime Festival on 5 & 6 May 2013._NEWLINE_On 23 May she was in the Bristol Channel en route to Gloucester where she arrived on 24 May for t\\nhe city\\'s Tall Ships Festival on 25 & 26 May, and was on the Gloucester and Sharpness Canal during June. She was at Gorey, Jersey on 22 July 2013 and had returned to Malta by October that year.\\n\\n_START_ARTICLE_\\nDolores Kuenz\\n_START_SECTION_\\nTable tennis career\\n_START_PARAGRAPH_\\nShe won a World Championship gold medal in the Women\\'s Team event at the 1937 World Table Tennis Championships known as the Corbillon Cup.  she was the captain of the team.\\n_START_SECTION_\\nHall of Fame\\n_START_PARAGRAPH_\\nShe was inducted into the USA Hall of Fame in 1979.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Remove special tags\n",
        "text = text.replace('_NEWLINE_', ' ')\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Tokenize words (split by whitespace)\n",
        "words = text.split()\n",
        "# Construct vocabulary (unique words)\n",
        "vocabulary = set(words)\n",
        "\n",
        "len(vocabulary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DKJIci_Tyb1",
        "outputId": "60616831-9f5c-4df9-845e-c510a2af50cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "419"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer to 2-1:\n",
        ">"
      ],
      "metadata": {
        "id": "PkC56-GFa486"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-2. Preprocess the three sentences in the provided \"universal_dependencies.txt\" and print the resulting vocabulary and report its length. Follow these instructions (4 points; 1 point for answer + 3 points for code):\n",
        "\n",
        "- Preprocess only the words in the tab-separated columns\n",
        "- Build a vocabulary by joining the lemma in the third column and the part-of-speech in the fourth column with an underscore (like *underlie\\_VERB*)\n",
        "- No meta information (like *\\# sent_id = GUM_academic_exposure-2*) should be included"
      ],
      "metadata": {
        "id": "yAyVO4uWY_GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for 2-2\n",
        "# the use of libraries is allowed\n",
        "file_path = '/content/universal_dependencies.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "  text = file.read()\n",
        "text"
      ],
      "metadata": {
        "id": "L1e9l707bRke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhTWSDLxZxt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer to 2-2 (just the length, the print output should be part of the code output above):\n",
        ">"
      ],
      "metadata": {
        "id": "8XwFOsttbURo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 (8 points)\n",
        "\n",
        "Objective: Question 3 tests your ability to interpret the data correctly and draw the correct conclusions.\n",
        "\n",
        "3-1. Your zoo enthusiast friend recently visited two zoos (Zoo 1 and Zoo 2) and built an animal-zoo frequency vector (analogous to term-document frequency vector) at each zoo. They share with you the following vocabulary and vector for each zoo:"
      ],
      "metadata": {
        "id": "3BgA6fdONKBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "vocab_zoo1 = [\"lion\", \"tiger\", \"elephant\", \"giraffe\", \"zebra\", \"panda\", \"kangaroo\", \"monkey\", \"wolf\", \"bear\"]\n",
        "vector_zoo1 = [23, 17, 11, 25, 9, 12, 1, 8, 15, 14]\n",
        "\n",
        "vocab_zoo2 = [\"dolphin\", \"giraffe\", \"monkey\", \"penguin\", \"crocodile\", \"bear\", \"hippopotamus\", \"shark\", \"wolf\", \"panda\"]\n",
        "vector_zoo2 = [17, 3, 2, 15, 22, 1, 8, 12, 9, 3]\n",
        "\n",
        "print(euclidean_distances([vector_zoo1, vector_zoo2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa9JNf0Km0zS",
        "outputId": "b80787a7-c8f4-41c0-e002-cdd97fd32915"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.         30.41381265]\n",
            " [30.41381265  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the output above, they conclude that the Euclidean distance of the two animal-zoo frequency vectors is about 30.41. Why is their reasoning incorrect (2 points)?\n",
        ">"
      ],
      "metadata": {
        "id": "NbtED1RipzZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Because two zoo frequency vectors have different vocabularies. The Euclidean distance calculation should be performed only on the common vocabulary, while for missing animals, their frequency should be treated as 0. They should use cosine similarity."
      ],
      "metadata": {
        "id": "QFkXS2WMbK3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-2. Calculate the correct Euclidean distance between the two vectors (2 points; 1 point for answer + 1 points for code)."
      ],
      "metadata": {
        "id": "49a9S3q3qsOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for 3-2\n",
        "# the use of libraries is allowed\n",
        "# you can assume that the variables from the code block above carries over\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Build the unified vocabulary\n",
        "all_animals = sorted(set(vocab_zoo1 + vocab_zoo2))  # Sorted for consistency\n",
        "\n",
        "# Create aligned frequency vectors\n",
        "vector1_aligned = [vector_zoo1[vocab_zoo1.index(animal)] if animal in vocab_zoo1 else 0 for animal in all_animals]\n",
        "vector2_aligned = [vector_zoo2[vocab_zoo2.index(animal)] if animal in vocab_zoo2 else 0 for animal in all_animals]\n",
        "\n",
        "# Compute the Euclidean distance\n",
        "euclidean_distance = np.sqrt(sum((np.array(vector1_aligned) - np.array(vector2_aligned)) ** 2))\n",
        "print(\"Correct Euclidean distance:\", euclidean_distance)\n"
      ],
      "metadata": {
        "id": "ZVsf71rfrOhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d0eb05-adc8-4a03-e2b3-92548601beaf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct Euclidean distance: 55.072679252057455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer to 3-2:\n",
        "> 55.07"
      ],
      "metadata": {
        "id": "YlUa6Pb7sEhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-3. Which three animals 'contribute most' to the Euclidean distance between the two vectors (which three animals have the biggest difference in frequency; 2 points; 1 point for answer + 1 points for code)?"
      ],
      "metadata": {
        "id": "iPKS-x-NsYN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for 3-3\n",
        "# the use of libraries is allowed\n",
        "# you can assume that the variables from the code block above carries over\n",
        "# Compute absolute differences\n",
        "diffs = np.abs(np.array(vector1_aligned) - np.array(vector2_aligned))\n",
        "\n",
        "# Get the indices of the top 3 contributors\n",
        "top_3_indices = np.argsort(diffs)[-3:]\n",
        "\n",
        "# Print the three animals with the highest contribution\n",
        "top_3_animals = [all_animals[i] for i in top_3_indices]\n",
        "print(\"Top 3 contributing animals:\", top_3_animals)\n"
      ],
      "metadata": {
        "id": "cTPPlKIwwpMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337c1e12-b9a2-4089-d993-d00bef07c41e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 contributing animals: ['crocodile', 'giraffe', 'lion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer to 3-3:\n",
        "> * crocodile\n",
        "* giraffe\n",
        "* lion"
      ],
      "metadata": {
        "id": "8hi34KJZwrs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-4. Based on your responses to 3-2 and 3-3, what reasonable conclusions can you draw about the difference between Zoo 1 and Zoo 2? Assume your friend correctly counted all the animals at the two Zoos (answer in 1-2 sentences; 2 points).\n",
        "> The difference between Zoo1 and Zoo2 is actually larger than my friend thinks. Also, the main difference doesn't come from the difference of species in two zoos. Instead, the number of animals such as crocodiles, giraffe, lions causes the main discrepancy."
      ],
      "metadata": {
        "id": "CrSzuCS0wvG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">"
      ],
      "metadata": {
        "id": "LQL8X7rRcvfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (6 points)\n",
        "\n",
        "Objective: Question 4 is about co-occurrence matrices, which are important for text analysis and NLP.\n",
        "\n",
        "4-1. Assume that the predefined `split_data` variable below is a sequence of preprocessed words. Using this data and the provided `word_index`, build and print two co-occurrence matrices using different context windows (4 points; 1 point for print output + 3 points for code).\n",
        "- Matrix 1: The context window is one word to the **right**.\n",
        "- Matrix 2: The context window is one word to the **left**.\n",
        "\n",
        "Hint: Adapt the code from Recitation #2, but be careful, because this is different from the lab session example, in which the context window was to both the left and the right.\n",
        "\n",
        "Warning: As `[\"A\", \"B\", \"C\", \"D\"]` occurs as both the word and the context, it may be easy to confuse what the rows and columns represent in the matrices. Not confusing this is important for 4-2 below."
      ],
      "metadata": {
        "id": "jORoH2n4NLIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build on the code below for 4-1\n",
        "# the use of libraries is allowed\n",
        "# use these pre-defined variables below\n",
        "raw_data = \"D D A B C D D A B D D D A B C D A B C A B C C C A B A B D A B D C A B C A B A B D C C D A B D A B A B D D D D C D C D A B A B A B D A B C C C C A B D C C C C D D D C A B C D A B C A B A B A B C D A B C D D C A B D A B D D A B A B C C A B C C D A B D A B A B C D D A B D C D A B D D A B D D C A B D A B C A B C D A B A B A B D A B C D A B D A B C D A B D A B C A B A B A B D A B D C A B D A B C D A B A B A B A B C D C A B C A B D D A B C D D C D C D D D D C C A B A B C D A B A B C C A B A B C A B D D D A B D C C D D D A B C C D C D A B C D D D D D D C C D C A B C C C D C D C A B A B A B D C D D A B A B A B D C A B C D D C A B A B A B A B D A B A B A B C C D A B C C C D D A B D C D A B A B A B D A B A B A B C A B D D C C C A B A B D A B D D D C D D D C A B D A B D A B C C A B C A B C C D A B A B D C A B D C A B D D C D D A B A B C D C D D C C C D D A B C A B D C C C C D A B A B A B C D C C D D C D D C A B A B D C A B D D D D C C D D A B D D C D C D C A B A B D A B D C D A B D A B A B C D A B D C A B C D D A B A B D D A B C C C C D C D D C A B C C D C A B D A B A B A B A B D A B C C C D D C C C D D D A B A B D D D D D C C C D A B D D C A B D A B D D C A B C D D D C D A B D C A B D C D C A B C A B A B D D D D D D C D D D D D D C C C D D C A B D C C D A B A B D A B D C A B C C A B A B A B A B C C\"\n",
        "split_data = raw_data.split()\n",
        "word_index = {word: idx for idx, word in enumerate([\"A\", \"B\", \"C\", \"D\"])}\n",
        "\n",
        "print(len(split_data))  # 655\n",
        "\n"
      ],
      "metadata": {
        "id": "dZqqaULa8twe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-2. Upon visual inspection of the matrices (you don't need to calculate any distances), are the representations for each word the same or different across the two matrices? Why or why not (2 points; 1 point for correctly identifying same/different + 1 point for reason)?\n",
        ">"
      ],
      "metadata": {
        "id": "EN55aPSyDbUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 (8 points)\n",
        "\n",
        "Objective: Question 5 is to prepare you for the final research project.\n",
        "\n",
        "5-1. Formulate a research question you would like to tackle for your final project. This will be evaluated in light of the FINER method introduced in the first lecture (2 points).\n",
        "> How do Federal Reserve statements and speeches influence market sentiment and liquidity, and can we develop an NLP-based predictive model to quantify these effects in the change of SPY price next period.\n",
        "\n",
        "5-2. In about 2-3 sentences, convince us why we should care about this research question. Can it reveal new scientific findings about the world? Can it lead to useful engineering applications (3 points)?\n",
        "> Understanding the impact of Federal Reserve communications on financial markets is crucial for policymakers, investors, and economists. By analyzing the language and tone of Fed statements, we can uncover hidden sentiment trends that drive market movements. This research can lead to practical applications in trading and economic forecasting by providing an early warning system for market fluctuations.\n",
        "\n",
        "\n",
        "5-3. In about 2-3 sentences, describe why this research question is best answered using text data, and what kind of text data you expect to use (3 points).\n",
        "> Because instead of certain numerical data, Federal Reserve statements, speeches, and meeting contain qualitative insights into economic outlooks and policy intentions. The data sources will include transcripts of FOMC meetings, official Fed communications, and financial news articles. By applying NLP techniques such as sentiment analysis, topic modeling, and text embeddings, we can quantify the impact of Fed communications on financial markets."
      ],
      "metadata": {
        "id": "2HzVbqhCNNen"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgyoV5hKXnXI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}